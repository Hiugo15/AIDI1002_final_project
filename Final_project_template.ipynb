{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5tr_jEBnh-jv"
   },
   "source": [
    "# Title: Quantitative comparison between Stable diffusion performance in human faces and cats\n",
    "\n",
    "#### Group Member Names : Hugo Garcia, Jesal Patel\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PeKSxMvrh-j0"
   },
   "source": [
    "### INTRODUCTION:\n",
    "*********************************************************************************************************************\n",
    "#### AIM : To evaluate the quality and realism of images generated by the Stable Diffusion model using cat images as a benchmark, and compute the Fréchet Inception Distance (FID) between generated and real cat images from the Google Open Images Dataset V7\n",
    "\n",
    "*********************************************************************************************************************\n",
    "#### Github Repo:\n",
    "Original https://github.com/aliborji/GFW\n",
    "Our project: https://github.com/Hiugo15/AIDI1002_final_project\n",
    "\n",
    "*********************************************************************************************************************\n",
    "#### DESCRIPTION OF PAPER: The paper conducts a comprehensive evaluation of three leading text-to-image diffusion models Stable Diffusion, Midjourney, and DALL·E 2 on tasks related to face generation. It introduces benchmarks that consider aesthetic quality, identity preservation, and realism. The authors also release a dataset and metrics to support future studies in generative image evaluation.\n",
    "\n",
    "*********************************************************************************************************************\n",
    "#### PROBLEM STATEMENT : Can the performance of a text-to-image generative model (specifically Stable Diffusion) be reliably evaluated using FID scores when the domain of interest shifts from human faces to non-human subjects (e.g., cats)? How well does the model generalize in generating high-quality images of non-human categories?\n",
    "\n",
    "*********************************************************************************************************************\n",
    "#### CONTEXT OF THE PROBLEM: Most benchmarks for diffusion models focus on human faces or aesthetic art. However, diffusion models are used in diverse real-world scenarios that include animals, objects, and scenes. Evaluating model performance on less human-centric content like cats provides insight into the generalization capacity of these models. Using the Google Open Images Dataset V7 and modifying the original paper's pipeline for local computation, we aim to test this capacity with cat images.\n",
    "*\n",
    "*********************************************************************************************************************\n",
    "#### SOLUTION: Use the Google Open Images Dataset V7 to extract real cat images, generate corresponding synthetic cat images using Stable Diffusion and carefully designed prompts, then calculate the FID score between real and generated images. The original code from the GFW paper is adapted for local execution with modified paths, prompt handling, and evaluation criteria.\n",
    "* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77PIPLQ-h-j1"
   },
   "source": [
    "# Background\n",
    "*********************************************************************************************************************\n",
    "\n",
    "\n",
    "|Reference|\n",
    "https://arxiv.org/abs/2210.00586\n",
    "\n",
    "Explanation|\n",
    "Provides code and methodology to evaluate diffusion models on face generation using FID and identity metrics.\n",
    "\n",
    "Dataset/Input|\n",
    "https://drive.google.com/file/d/16BXO1fgN08UGLLeA5ZNU9bhwAkcAOdci/view\n",
    "\n",
    "Weakness|\n",
    "Focused only on human faces; needs adaptation for other subjects like animals.\n",
    "\n",
    "\n",
    "\n",
    "*********************************************************************************************************************\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deODH3tMh-j2"
   },
   "source": [
    "# Implement paper code :\n",
    "*********************************************************************************************************************\n",
    "This code was modified by us to be run using local storage and Jupyter notebook environment\n",
    "*\n",
    "\n",
    "!pip install torch torchvision numpy scipy pandas pillow tqdm\n",
    "\n",
    "# ====================================\n",
    "# STEP 1: IMPORT LIBRARIES\n",
    "# ====================================\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from scipy import linalg\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# STEP 2: DEFINE PATHS (LOCAL SETUP)\n",
    "# ====================================\n",
    "real_path = r'C:\\Users\\hugo_\\Documents\\hugo_garcia\\Big_Data_Analytics_georgian_college\\AI management\\1 semester\\Machine Learning Programming\\Final Project\\datasets\\real\\real_faces'\n",
    "sd_path = r'C:\\Users\\hugo_\\Documents\\hugo_garcia\\Big_Data_Analytics_georgian_college\\AI management\\1 semester\\Machine Learning Programming\\Final Project\\datasets\\stable_diffusion\\generated_data\\faces_generated'\n",
    "mj_path = r'C:\\Users\\hugo_\\Documents\\hugo_garcia\\Big_Data_Analytics_georgian_college\\AI management\\1 semester\\Machine Learning Programming\\Final Project\\datasets\\midjourney\\faces_generated_midjourney'\n",
    "dalle_path = r'C:\\Users\\hugo_\\Documents\\hugo_garcia\\Big_Data_Analytics_georgian_college\\AI management\\1 semester\\Machine Learning Programming\\Final Project\\datasets\\dalle2\\DALLEFaces'\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# STEP 3: IMAGE LOADING FUNCTION\n",
    "# ====================================\n",
    "def get_activations(folder, model, batch_size=50, max_images=2000):\n",
    "    model.eval()\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((299, 299)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5]*3, std=[0.5]*3),\n",
    "    ])\n",
    "\n",
    "    images = []\n",
    "    image_files = [f for f in os.listdir(folder) if f.lower().endswith(('jpg', 'png', 'jpeg'))]\n",
    "    image_files = image_files[:max_images]  # limit number of images\n",
    "\n",
    "    for file in tqdm(image_files, desc=f'Loading images from {os.path.basename(folder)}'):\n",
    "        img_path = os.path.join(folder, file)\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img = preprocess(img)\n",
    "        images.append(img)\n",
    "\n",
    "    images = torch.stack(images)\n",
    "    activations = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(images), batch_size):\n",
    "            batch = images[i:i+batch_size]\n",
    "            output = model(batch)\n",
    "            activations.append(output.cpu().numpy())\n",
    "\n",
    "    activations = np.concatenate(activations, axis=0)\n",
    "    return activations\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# STEP 4: LOAD INCEPTIONV3 MODEL\n",
    "# ====================================\n",
    "from torchvision.models import inception_v3, Inception_V3_Weights\n",
    "\n",
    "weights = Inception_V3_Weights.DEFAULT\n",
    "inception = inception_v3(weights=weights, aux_logits=True)  \n",
    "inception.fc = nn.Identity()\n",
    "inception.eval()\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# STEP 5: FID CALCULATION FUNCTION\n",
    "# ====================================\n",
    "def calculate_fid(act1, act2):\n",
    "    mu1, sigma1 = act1.mean(axis=0), np.cov(act1, rowvar=False)\n",
    "    mu2, sigma2 = act2.mean(axis=0), np.cov(act2, rowvar=False)\n",
    "\n",
    "    diff = mu1 - mu2\n",
    "    covmean, _ = linalg.sqrtm(sigma1 @ sigma2, disp=False)\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    fid = diff @ diff + np.trace(sigma1 + sigma2 - 2 * covmean)\n",
    "    return fid\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# STEP 6: LOAD ACTIVATIONS\n",
    "# ====================================\n",
    "print(\"Extracting activations...\")\n",
    "\n",
    "act_real = get_activations(real_path, inception)\n",
    "act_sd = get_activations(sd_path, inception)\n",
    "act_mj = get_activations(mj_path, inception)\n",
    "act_dalle = get_activations(dalle_path, inception)\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# STEP 7: CALCULATE FID SCORES\n",
    "# ====================================\n",
    "print(\"\\nCalculating FID Scores...\\n\")\n",
    "\n",
    "fid_sd = calculate_fid(act_real, act_sd)\n",
    "fid_mj = calculate_fid(act_real, act_mj)\n",
    "fid_dalle = calculate_fid(act_real, act_dalle)\n",
    "\n",
    "print(f'FID (Stable Diffusion vs Real): {fid_sd:.2f}')\n",
    "print(f'FID (Midjourney vs Real): {fid_mj:.2f}')\n",
    "print(f'FID (DALL·E 2 vs Real): {fid_dalle:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gkHhku9h-j2"
   },
   "source": [
    "*********************************************************************************************************************\n",
    "### Contribution  Code :\n",
    "*\n",
    "\n",
    "Code to generate images using stable diffusion:\n",
    "\n",
    "!pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118  # or use cu117 for older GPUs\n",
    "!pip install diffusers[torch] transformers accelerate --upgrade\n",
    "\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Replace YOUR_HUGGINGFACE_TOKEN with your actual token\n",
    "login(token=\"YOUR_HUGGINGFACE_TOKEN\")\n",
    "\n",
    "\n",
    "\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    safety_checker=None,  \n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    ")\n",
    "pipe = pipe.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "output_dir = r\"C:\\Users\\hugo_\\Documents\\hugo_garcia\\Big_Data_Analytics_georgian_college\\AI management\\1 semester\\Machine Learning Programming\\Final Project\\datasets\\Project_Cat_images\\Stable_Diffusion\\Generated_images\"\n",
    "prompt = \"A cat, centered, high detail, studio lighting, DSLR\"\n",
    "num_images = 100\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "print(f\"Generating {num_images} images...\")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for i in tqdm(range(num_images)):\n",
    "        image = pipe(prompt, num_inference_steps=20, height=384, width=384).images[0]\n",
    "        image.save(os.path.join(output_dir, f\"cat_sd_{i:03d}.png\"))\n",
    "\n",
    "print(f\"✅ Done! All images saved to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to run the new comparison:\n",
    "\n",
    "# ====================================\n",
    "# STEP 1: IMPORT LIBRARIES\n",
    "# ====================================\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from scipy import linalg\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# STEP 2: DEFINE PATHS (LOCAL SETUP)\n",
    "# ====================================\n",
    "real_path = r'C:\\Users\\hugo_\\Documents\\hugo_garcia\\Big_Data_Analytics_georgian_college\\AI management\\1 semester\\Machine Learning Programming\\Final Project\\datasets\\Project_Cat_images\\Real_data\\Real_images\\content\\OID\\Dataset\\train\\Cat\\images'\n",
    "sd_path = r'C:\\Users\\hugo_\\Documents\\hugo_garcia\\Big_Data_Analytics_georgian_college\\AI management\\1 semester\\Machine Learning Programming\\Final Project\\datasets\\Project_Cat_images\\Stable_Diffusion\\Generated_images'\n",
    "\n",
    "# ====================================\n",
    "# STEP 3: IMAGE LOADING FUNCTION\n",
    "# ====================================\n",
    "def get_activations(folder, model, batch_size=50, max_images=2000):\n",
    "    model.eval()\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((299, 299)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5]*3, std=[0.5]*3),\n",
    "    ])\n",
    "\n",
    "    images = []\n",
    "    image_files = [f for f in os.listdir(folder) if f.lower().endswith(('jpg', 'png', 'jpeg'))]\n",
    "    image_files = image_files[:max_images]  # limit number of images\n",
    "\n",
    "    for file in tqdm(image_files, desc=f'Loading images from {os.path.basename(folder)}'):\n",
    "        img_path = os.path.join(folder, file)\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img = preprocess(img)\n",
    "        images.append(img)\n",
    "\n",
    "    images = torch.stack(images)\n",
    "    activations = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(images), batch_size):\n",
    "            batch = images[i:i+batch_size]\n",
    "            output = model(batch)\n",
    "            activations.append(output.cpu().numpy())\n",
    "\n",
    "    activations = np.concatenate(activations, axis=0)\n",
    "    return activations\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# STEP 4: LOAD INCEPTIONV3 MODEL\n",
    "# ====================================\n",
    "from torchvision.models import inception_v3, Inception_V3_Weights\n",
    "\n",
    "weights = Inception_V3_Weights.DEFAULT\n",
    "inception = inception_v3(weights=weights, aux_logits=True)  \n",
    "inception.fc = nn.Identity()\n",
    "inception.eval()\n",
    "\n",
    "\n",
    "# STEP 5: FID CALCULATION FUNCTION\n",
    "# ====================================\n",
    "def calculate_fid(act1, act2):\n",
    "    mu1, sigma1 = act1.mean(axis=0), np.cov(act1, rowvar=False)\n",
    "    mu2, sigma2 = act2.mean(axis=0), np.cov(act2, rowvar=False)\n",
    "\n",
    "    diff = mu1 - mu2\n",
    "    covmean, _ = linalg.sqrtm(sigma1 @ sigma2, disp=False)\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    fid = diff @ diff + np.trace(sigma1 + sigma2 - 2 * covmean)\n",
    "    return fid\n",
    "\n",
    "# ====================================\n",
    "# STEP 6: LOAD ACTIVATIONS\n",
    "# ====================================\n",
    "print(\"Extracting activations...\")\n",
    "\n",
    "act_real = get_activations(real_path, inception)\n",
    "act_sd = get_activations(sd_path, inception)\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# STEP 7: CALCULATE FID SCORES\n",
    "# ====================================\n",
    "print(\"\\nCalculating FID Scores...\\n\")\n",
    "\n",
    "fid_sd = calculate_fid(act_real, act_sd)\n",
    "fid_mj = calculate_fid(act_real, act_mj)\n",
    "fid_dalle = calculate_fid(act_real, act_dalle)\n",
    "\n",
    "print(f'FID (Stable Diffusion vs Real): {fid_sd:.2f}')\n",
    "print(f'FID (Midjourney vs Real): {fid_mj:.2f}')\n",
    "print(f'FID (DALL·E 2 vs Real): {fid_dalle:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YdFCgWoh-j3"
   },
   "source": [
    "### Results :\n",
    "\n",
    "After adapting and running the original GFW notebook locally with modifications for dataset location and evaluation category, we computed the Fréchet Inception Distance (FID) between 100 cat images generated by Stable Diffusion and real cat images from the Google Open Images Dataset V7.\n",
    "\n",
    "✅ FID (Stable Diffusion vs Real): 187.67\n",
    "\n",
    "This FID score reflects the statistical difference between the distributions of real and generated cat images in the embedding space of an Inception network. A lower FID indicates more similarity and higher visual fidelity.\n",
    "\n",
    "*******************************************************************************************************************************\n",
    "\n",
    "\n",
    "\n",
    "#### Observations :\n",
    "\n",
    "The FID score of 187.67 is relatively high, suggesting a noticeable difference between real and generated cat images.\n",
    "\n",
    "This outcome may be influenced by:\n",
    "\n",
    "The relatively small sample size of only 100 generated images, which limits statistical robustness.\n",
    "\n",
    "The fact that Stable Diffusion is not explicitly trained on cats, especially with the kind of detail and consistency required for FID-based evaluation.\n",
    "\n",
    "Variability in prompts and possible divergence in style between generated images and real-world, natural images.\n",
    "\n",
    "Despite this, the model still managed to generate semantically correct and recognizable cat images using only text prompts like \"a photo of a cat sitting on a couch\".\n",
    "\n",
    "Qualitatively, many generated images show stylistic artifacts or ambiguous shapes, indicating that fine-tuning or improved prompt engineering might be needed for better results.\n",
    "\n",
    "*******************************************************************************************************************************\n",
    "*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3JVj9dKh-j3"
   },
   "source": [
    "### Conclusion and Future Direction :\n",
    "*******************************************************************************************************************************\n",
    "#### Learnings :\n",
    "\n",
    "We successfully adapted an existing research benchmark to evaluate text-to-image diffusion models on non-human subjects, specifically cats.\n",
    "\n",
    "The process involved working with large image datasets, modifying code for local execution, and understanding how FID reflects the quality of generated images.\n",
    "\n",
    "This adaptation also demonstrated the importance of prompt quality, image diversity, and dataset consistency when evaluating generative models.\n",
    "\n",
    "*******************************************************************************************************************************\n",
    "#### Results Discussion :\n",
    "\n",
    "The FID score of 187.67 suggests that Stable Diffusion's outputs were perceptually and statistically less similar to real cat images compared to typical human face generation tasks.\n",
    "\n",
    "Limited by hardware resources, we only generated 100 images, which affects the reliability of FID. A larger sample (e.g., 1,000 or more) would yield a more stable and accurate metric.\n",
    "\n",
    "Additionally, while FID captures feature-space similarity, it does not account for semantic correctness—many generated cats were indeed recognizable but contained subtle visual distortions.\n",
    "\n",
    "\n",
    "*******************************************************************************************************************************\n",
    "#### Limitations :\n",
    "\n",
    "The evaluation was constrained to 100 generated images due to hardware and runtime limitations. This low number can inflate FID variance.\n",
    "\n",
    "We did not compare against Midjourney or DALL·E 2 in this phase, as those models are not open-source and require API access.\n",
    "\n",
    "Prompt engineering was basic (single-line prompts) and not fine-tuned for realism or stylistic coherence.\n",
    "\n",
    "The Stable Diffusion model used was not fine-tuned on cat-specific imagery or conditioned on segmentation/masks.\n",
    "\n",
    "*******************************************************************************************************************************\n",
    "#### Future Extension :\n",
    "\n",
    "Generate a larger dataset (e.g., 1,000+ images) to compute a more statistically meaningful FID score.\n",
    "\n",
    "Experiment with better prompts or use prompt ensembles to improve generation quality and consistency.\n",
    "\n",
    "Apply CLIP-based filtering to select the best generated outputs before computing FID.\n",
    "\n",
    "Extend the comparison to include Midjourney and DALL·E 2 (via API), and evaluate them under the same metrics.\n",
    "\n",
    "Investigate other evaluation metrics like Inception Score (IS), CLIPScore, or user studies to complement FID.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATXtFdtBh-j4"
   },
   "source": [
    "# References:\n",
    "\n",
    "[1]:  Research paper\n",
    "https://arxiv.org/abs/2210.00586\n",
    "\n",
    "[2]:GitHub repository of the research paper\n",
    "https://github.com/aliborji/GFW\n",
    "\n",
    "[3]:Datasets used in the research paper\n",
    "https://drive.google.com/file/d/16BXO1fgN08UGLLeA5ZNU9bhwAkcAOdci/view\n",
    "\n",
    "[4]: Google images dataset\n",
    "https://storage.googleapis.com/openimages/web/index.html\n",
    "\n",
    "[5]: Instructions on how to download from google images\n",
    "https://sharmaji27.medium.com/easiest-way-to-download-data-from-the-open-image-dataset-553dccfb92d8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQnMSAf-h-j4"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
